{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Инструменты\n",
    "\n",
    "Импортнем все необходимое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Данные\n",
    "\n",
    "В качестве корпуса я взяла корпус субтитров к 9 сезону реалити-шоу \"RuPaul's Drag Race\". Он содержит субтитры к 13 эпизодам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/deltamachine/Desktop/projects/rupaul-subtitles-analysis/subtitles/s9/'\n",
    "\n",
    "files = os.listdir(path)\n",
    "corpus = []\n",
    "\n",
    "for file in files:\n",
    "    with open(path + file, 'r', encoding=\"utf-8\") as file:\n",
    "        corpus.append(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Препроцессинг\n",
    "Поступим так:\n",
    "\n",
    "1) Переведем текст в нижний регистр\n",
    "\n",
    "2) Выбросим все теги, нечитаемые символы, специфические символы типа ♪, а также отметки времени (корпус - это файлы в формате .srt, т.е. реальные непочищенные субтитры), а также пунктуацию и лишние пробелы\n",
    "\n",
    "3) Токенизируем и лемматизируем текст.\n",
    "\n",
    "4) Отсеем стоп-слова, причем не только те, которые нам предлагает NLTK, но и локальные - для этого будем выкидывать 15 самых частотных слов в эпизоде, а также составлять общий список \"локальных\" стоп-слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = punctuation + '«»—…“”*№–'\n",
    "all_local_stops = set()\n",
    "stops = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text, lang):\n",
    "    global all_local_stops \n",
    "    global stops\n",
    "    \n",
    "    if lang == 'en':\n",
    "        stops = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    elif lang == 'ru':\n",
    "        stops = set(stopwords.words('russian'))\n",
    "        lemmatizer = Mystem()\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('[0-9]+:[0-9]+.*?\\n', ' ', text)\n",
    "    text = re.sub('<.*?>', ' ', text)\n",
    "    text = re.sub('\\ufeff1', ' ', text)\n",
    "    text = re.sub('\\n[0-9]*', ' ', text)\n",
    "    text = re.sub('♪', ' ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "    text = [word for word in word_tokenize(text) if word not in punct and word not in stops]  \n",
    "    \n",
    "    if lang == 'en': \n",
    "        text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    elif lang == 'ru':\n",
    "        text = [lemmatizer.lemmatize(word)[0] for word in text]\n",
    "    \n",
    "    local_stops = [elem[0] for elem in Counter(text).most_common(15)]\n",
    "    all_local_stops |= set(local_stops)\n",
    "    \n",
    "    text = ' '.join([word for word in text if word not in local_stops])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_corpus = []\n",
    "\n",
    "for text in corpus:\n",
    "    normalized_text = normalize_text(text, 'en')\n",
    "    normalized_corpus.append(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Извлечение ключевых слов\n",
    "\n",
    "Конкретно для этого корпуса большой смысл имеет TF-IDF - надо ведь посмотреть, чем конкретно эпизоды отличаются друг от друга.\n",
    "\n",
    "Применим его и выведем по 5 ключевых слов для каждого эпизода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops |= all_local_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords_extraction(corpus, stops):\n",
    "    vectorizer = TfidfVectorizer(stop_words=stops)\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    id2word = {i:word for i,word in enumerate(vectorizer.get_feature_names())} \n",
    "\n",
    "    for text_row in range(matrix.shape[0]):\n",
    "        row_data = matrix.getrow(text_row) \n",
    "        all_words = row_data.toarray().argsort() \n",
    "        top_words = all_words[0,:-6:-1] \n",
    "        print([id2word[w] for w in top_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['coo', 'shack', 'jaymes', 'floozy', '52']\n",
      "['dah', 'gah', 'kah', 'category', 'four']\n",
      "['pageant', 'vega', 'see', 'new', 'video']\n",
      "['charlie', 'naya', 'eureka', 'two', 'show']\n",
      "['duncan', 'crew', 'sarge', 'heel', 'sister']\n",
      "['monna', 'dartin', 'mom', 'prom', 'action']\n",
      "['greedy', 'club', 'okay', 'pilot', 'kid']\n",
      "['roast', 'burning', 'farrah', 'okay', 'time']\n",
      "['toot', 'fan', 'boot', 'valentina', 'lip']\n",
      "['inspired', 'aesthetic', 'kimora', 'city', 'season']\n",
      "['madonna', 'jasmine', 'marlene', 'snatch', 'dietrich']\n",
      "['macho', 'rainbow', 'unicorn', 'village', 'rhythmic']\n",
      "['starfish', 'sidekick', 'aquapussy', 'fire', 'hero']\n"
     ]
    }
   ],
   "source": [
    "keywords_extraction(normalized_corpus, stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты достаточно осмысленные - по ключевым словам довольно легко угадать эпизод.\n",
    "\n",
    "Например, массив ['duncan', 'crew', 'sarge', 'heel', 'sister'] описывает эпизод , в котором принимали участие члены съемочной группы (\"сrew\") - в частности, люди, которых звали Duncan и Sarge.\n",
    "\n",
    "А массив ['madonna', 'jasmine', 'marlene', 'snatch', 'dietrich'] описывает эпизод с челленджем под названием \"Snatch Game\", в котором неоднократно упоминались Марлен Дитрих и Джасмин Мастерс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Обработка другого датасета\n",
    "\n",
    "Возьмем корпус текстов Russia Today и посмотрим, сколько в нем текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = []\n",
    "\n",
    "with open('/home/deltamachine/Downloads/russia_today_0.jsonlines', 'r') as file:\n",
    "    for line in file.readlines():\n",
    "        test_corpus.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я уже не укладываюсь в дедлайн и процессинг всех текстов займет очень много времени - поэтому я просто выберу b обработаю 13 случайных, по аналогии с 13 файлами в исходном корпусе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_local_stops = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_test_corpus = []\n",
    "indexes = random.sample(range(0, 1000), 13)\n",
    "\n",
    "for i in indexes:\n",
    "    normalized_test_text = normalize_text(test_corpus[i]['content'], 'ru')\n",
    "    normalized_test_corpus.append(normalized_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "774"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['биатлон',\n",
       "  'кёрлинг',\n",
       "  'лыжный спорт',\n",
       "  'ои-2018 биатлон',\n",
       "  'ои-2018 конькобежный спорт'],\n",
       " ['алеппо', 'боевики', 'в мире', 'сирия', 'видео'],\n",
       " ['зарплата', 'здоровье', 'медицина', 'новости россии', 'политика'],\n",
       " ['в мире', 'вашингтон', 'дональд трамп', 'кндр', 'ким чен ын'],\n",
       " ['австралия', 'в мире', 'джеймс мэттис', 'дональд трамп', 'кндр'],\n",
       " ['бокс', 'в мире', 'россия', 'скандал', 'спорт'],\n",
       " ['евразийский союз', 'еврокомиссия', 'конституция', 'национализм', 'польша'],\n",
       " ['бокс', 'россияне', 'сша', 'сергей ковалёв', 'спорт'],\n",
       " ['в мире',\n",
       "  'дональд трамп',\n",
       "  'исламское государство',\n",
       "  'кндр',\n",
       "  'конфликт сша и кндр'],\n",
       " ['автомобиль', 'в россии', 'гибдд', 'деньги', 'новости россии'],\n",
       " ['интервью', 'магнус карлсен', 'сергей карякин', 'спорт', 'шахматы'],\n",
       " ['безопасность', 'евровидение', 'конкурс', 'крым', 'песня'],\n",
       " ['видео', 'геноцид армян', 'кино', 'культура']]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_keywords = []\n",
    "\n",
    "for i in indexes:\n",
    "    true_keywords.append(test_corpus[i]['keywords'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops |= all_local_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['медаль', 'шанс', 'альпензия', 'шорт', 'квалификация']\n",
      "['join', 'ярошевский', 'интеграция', 'информированность', 'информация']\n",
      "['медик', 'волновать', 'персонал', 'хромов', 'лекарство']\n",
      "['асмолов', 'jwplayer', 'ga', 'players', 'трамп']\n",
      "['корейский', 'недовольство', 'путем', 'адмирал', 'белый']\n",
      "['сборная', 'бойкотировать', 'кремлев', 'соревнование', 'украинец']\n",
      "['брюссель', 'ек', 'качиньский', 'еврокомиссия', 'справедливость']\n",
      "['раунд', 'оппонент', 'боксер', 'отдавать', 'бокс']\n",
      "['нескромный', 'корея', 'пхеньян', 'рейган', 'влияние']\n",
      "['рубль', 'транспортный', 'совершать', 'средство', 'предусматривать']\n",
      "['ваш', 'карлсен', 'карякин', 'ребенок', 'игра']\n",
      "['въезд', 'приезжать', 'проект', 'певица', 'участница']\n",
      "['актриса', 'оскар', 'национальность', 'звезда', 'французский']\n"
     ]
    }
   ],
   "source": [
    "keywords_extraction(normalized_test_corpus, stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...Видно, что ключевые слова совпадают плохо. Во многом, потому, что в \"оригинальных\" ключевых словах много словосочетаний. И потому, что алгоритм выдает 5 самых \"специфичных\" слов, а в тестовом корпусе у нас нет возможности отобрать 5 самых специфичных из уже определенных ключевых слов, поэтому мы взяли 5 первых (довольно тупо, но что уж поделать).\n",
    "\n",
    "А еще, возможно, для этого корпуса не имело смысла удалять самые частотные слова после исключения стоп-слов (а для исходного корпуса - однозначно имело). Мораль: подходи к каждым данным индивидуально!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Оценка результатов\n",
    "\n",
    "По причинам, указанным выше, прогонять функцию оценки не особо имеет смысл, но я ее все-таки напишу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(true_keys, predicted_keys):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_keys)):\n",
    "        true_keys = set(true_keys[i])\n",
    "        predicted_kw = set(predicted_keys[i])\n",
    "        \n",
    "        tp = len(true_keys & predicted_keys)\n",
    "        union = len(true_keys | predicted_keys)\n",
    "        fp = len(predicted_keys - true_keys)\n",
    "        fn = len(true_keys - predicted_keys)\n",
    "        \n",
    "        if (tp + fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp + fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "            \n",
    "        if (prec + rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2 * (prec * rec)) / (prec + rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "        \n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
