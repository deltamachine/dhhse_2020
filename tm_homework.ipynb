{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Инструменты\n",
    "\n",
    "Импортируем все необходимое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from pymystem3 import Mystem\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Данные\n",
    "\n",
    "В качестве корпуса я взяла англоязычный корпус субтитров к реалити-шоу \"RuPaul's Drag Race\". Он содержит субтитры к 108 эпизодам из 2, 3, 4, 5, 6, 7, 8 и 9 сезонов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/deltamachine/Desktop/projects/rupaul-subtitles-analysis/subtitles/'\n",
    "\n",
    "season_dirs = os.listdir(path)\n",
    "corpus = []\n",
    "\n",
    "for directory in season_dirs:\n",
    "    episodes = os.listdir(path + directory)\n",
    "    \n",
    "    for ep in episodes:\n",
    "        ep_path = '%s%s/%s' % (path, directory, ep)\n",
    "        \n",
    "        with open(ep_path, 'r', encoding=\"utf-8\") as file:\n",
    "            corpus.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Препроцессинг\n",
    "Поступим так:\n",
    "\n",
    "1) Переведем текст в нижний регистр\n",
    "\n",
    "2) Выбросим все теги, нечитаемые символы, специфические символы типа ♪, а также отметки времени (корпус - это файлы в формате .srt, т.е. реальные непочищенные субтитры), а также пунктуацию и лишние пробелы\n",
    "\n",
    "3) Токенизируем и лемматизируем текст.\n",
    "\n",
    "4) Отсеем стоп-слова, причем не только те, которые нам предлагает NLTK, но и локальные - для этого будем выкидывать 15 самых частотных слов в эпизоде, а также составлять общий список \"локальных\" стоп-слов. Также добавим к этим стоп-словам небольшой список несвязных междометий, которые, на первый взгляд, часто встречаются в корпусе.\n",
    "\n",
    "5) Оставим только существительные - при топик моделлинге часто так поступают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = punctuation + '«»—…“”*№–'\n",
    "all_local_stops = set(['uh', 'mm', 'la', 'yi', 'hmm', 'whoo', 'ah', 'oh', 'hoo', 've', 'ho', 'blah'])\n",
    "stops = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_text(text, lang):\n",
    "    global all_local_stops \n",
    "    global stops\n",
    "    \n",
    "    if lang == 'en':\n",
    "        stops = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "    elif lang == 'ru':\n",
    "        stops = set(stopwords.words('russian'))\n",
    "        lemmatizer = Mystem()\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('[0-9]+:[0-9]+.*?\\n', ' ', text)\n",
    "    text = re.sub('<.*?>', ' ', text)\n",
    "    text = re.sub('\\ufeff1', ' ', text)\n",
    "    text = re.sub('\\n[0-9]*', ' ', text)\n",
    "    text = re.sub('♪', ' ', text)\n",
    "    text = re.sub('\\s{2,}', ' ', text)\n",
    "\n",
    "    text = [word for word in word_tokenize(text) if word not in punct and word not in stops]  \n",
    "    \n",
    "    if lang == 'en': \n",
    "        text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    elif lang == 'ru':\n",
    "        text = [lemmatizer.lemmatize(word)[0] for word in text]\n",
    "    \n",
    "    local_stops = [elem[0] for elem in Counter(text).most_common(15)]\n",
    "    all_local_stops |= set(local_stops)\n",
    "    \n",
    "    tagged_text = pos_tag(text)\n",
    "    \n",
    "    text = [word[0] for word in tagged_text if word[0] not in local_stops\n",
    "                                                     and word[1] == 'NN']\n",
    "    \n",
    "    text = list(set(text))\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_corpus = []\n",
    "\n",
    "for text in corpus:\n",
    "    normalized_text = normalize_text(text, 'en')\n",
    "    normalized_corpus.append(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Topic modelling: cпособ 1\n",
    "\n",
    "Сначала попробуем использовать TfIdf в сочетании с латентным размещением Дирихле из пакета sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops |= all_local_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stops)\n",
    "matrix = vectorizer.fit_transform(normalized_corpus)\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_modelling_sklearn(vectorizer, matrix, topics_num, words_num):\n",
    "    lda = LDA(n_components=topics_num, n_jobs=-1, learning_method='online')\n",
    "    lda.fit(matrix)\n",
    "\n",
    "    for topic_index, topic in enumerate(lda.components_):\n",
    "        print('\\nTopic %s:' % str(topic_index + 1))\n",
    "        print(' '.join([words[i] for i in topic.argsort()[:-words_num - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем выделить 10 тем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "bulge mortgage chill constituency sarge woman river\n",
      "\n",
      "Topic 2:\n",
      "lip okay hi decision kind man body\n",
      "\n",
      "Topic 3:\n",
      "roulez bed husband amy tchotchke grocery snake\n",
      "\n",
      "Topic 4:\n",
      "lip god week cover side thought someone\n",
      "\n",
      "Topic 5:\n",
      "launch chance painting slit hutton tina dreamt\n",
      "\n",
      "Topic 6:\n",
      "tone terrible level binge nobody drinking group\n",
      "\n",
      "Topic 7:\n",
      "hen whory breathy sleeve rescue mix triple\n",
      "\n",
      "Topic 8:\n",
      "body knee anything beyoncé hope gayestballever construction\n",
      "\n",
      "Topic 9:\n",
      "poker coal stepsister burn comfortable summer jordin\n",
      "\n",
      "Topic 10:\n",
      "cue heterosexuality pitcher hardwood tell sunset fixin\n"
     ]
    }
   ],
   "source": [
    "topics_num = 10\n",
    "words_num = 7\n",
    "\n",
    "topic_modelling_sklearn(vectorizer, matrix, topics_num, words_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не особо вижу смысл.\n",
    "\n",
    "Попробуем выделить 3 темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "man santino represent word cocktail woman club\n",
      "\n",
      "Topic 2:\n",
      "hi lip okay body decision man kind\n",
      "\n",
      "Topic 3:\n",
      "runway mr model supply face cover serve\n"
     ]
    }
   ],
   "source": [
    "topics_num = 3\n",
    "words_num = 7\n",
    "\n",
    "topic_modelling_sklearn(vectorizer, matrix, topics_num, words_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последняя тема в принципе содержит в себе лексику, использующуюся во время презентаций нарядов на подиуме (runway, face, serve, \"Cover Girl\" - название песни, которая часто играет на подиуме), но первые две темы как-то смысла не несут.\n",
    "\n",
    "...или 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 1:\n",
      "crip miranda resurrection countless salt legacy allude\n",
      "\n",
      "Topic 2:\n",
      "bot ageless motel sarah follicle clothing albert\n",
      "\n",
      "Topic 3:\n",
      "hug bye pride president parent nail lip\n",
      "\n",
      "Topic 4:\n",
      "sober corset ruco scarf ex minj dance\n",
      "\n",
      "Topic 5:\n",
      "boo please fire supply engine charisma lifetime\n",
      "\n",
      "Topic 6:\n",
      "creator oscar masculine carrera ruffle tatianna wink\n",
      "\n",
      "Topic 7:\n",
      "vacuum living baloney self twice film budget\n",
      "\n",
      "Topic 8:\n",
      "trinity bag damn duck city date business\n",
      "\n",
      "Topic 9:\n",
      "jewel network audience element television righty volume\n",
      "\n",
      "Topic 10:\n",
      "shoot capulet flaunt kat bird ruche disaster\n",
      "\n",
      "Topic 11:\n",
      "kamaru rachael jockey parent dryer eloquent performing\n",
      "\n",
      "Topic 12:\n",
      "bitchfest leash else silicone facet quick lone\n",
      "\n",
      "Topic 13:\n",
      "lan cigarette girlfriend essence lol gate scene\n",
      "\n",
      "Topic 14:\n",
      "scale demand peron wad travel squeak pa\n",
      "\n",
      "Topic 15:\n",
      "impersonation buff concert doubt wootity silicone getup\n",
      "\n",
      "Topic 16:\n",
      "davenport exit bed apparent smudge frail heart\n",
      "\n",
      "Topic 17:\n",
      "gender eyelash happen conversation doll amy goyls\n",
      "\n",
      "Topic 18:\n",
      "zamochkova avenue union weight lead eleganza dustin\n",
      "\n",
      "Topic 19:\n",
      "kack june drumroll cause shirt bullet page\n",
      "\n",
      "Topic 20:\n",
      "shoot crazy shine fool end john cause\n",
      "\n",
      "Topic 21:\n",
      "dah dun boom smurf gah blackie terror\n",
      "\n",
      "Topic 22:\n",
      "bone maker tease deliberate terrific virgin colorevolution\n",
      "\n",
      "Topic 23:\n",
      "rooker macho operation reunion squatty lipstick wikipedia\n",
      "\n",
      "Topic 24:\n",
      "syncs perfect ody casting congratulation mouthing executive\n",
      "\n",
      "Topic 25:\n",
      "interview language pray entertainer compete mouth doll\n"
     ]
    }
   ],
   "source": [
    "topics_num = 25\n",
    "words_num = 7\n",
    "\n",
    "topic_modelling_sklearn(vectorizer, matrix, topics_num, words_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все по-прежнему плохо, но уже чуть интереснее. Тема 21 выглядит как звукоподражания ('dun', 'dah', 'gah', 'boom'). Тема 5 собрала в себе кусочки фраз, повторяющихся в каждом эпизоде: lifetime supply (of cosmetics) в качестве одного из призов победителю, \"charisma, uniqueness, nerve and talent\" и \"gentlemen, start your engines!\". Тема 9 содержит лексику, употребляющуюся на финалах (там и network, и audience, и транслируется все на television, и призы от компании Fierce Drag Jewels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Topic modelling: способ 2\n",
    "\n",
    "Сейчас попробуем использовать инструменты из пакета gensim - там есть свой алгоритм латентного размещения Дирихле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalized_corpus_lda = [elem.split() for elem in normalized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = Dictionary(normalized_corpus_lda)\n",
    "lda_corpus = [dictionary.doc2bow(text) for text in normalized_corpus_lda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_modelling_gensim(lda_corpus, dictionary, topics_num, words_num):\n",
    "    lda = LdaModel(lda_corpus, id2word=dictionary, num_topics=topics_num)\n",
    "\n",
    "    topics = lda.show_topics(num_topics=topics_num, num_words=words_num, formatted=False)\n",
    "    topics = [(topic[0], [word[0] for word in topic[1]]) for topic in topics]\n",
    "\n",
    "    for num, words in topics:\n",
    "        print('\\nTopic %s:' % (str(num)))\n",
    "        print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала на 10 темах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "competition name stage anything time today chance\n",
      "\n",
      "Topic 1:\n",
      "god music woman decision feel hello competition\n",
      "\n",
      "Topic 2:\n",
      "something winner thing challenge engine guy woman\n",
      "\n",
      "Topic 3:\n",
      "day today thing part face lady game\n",
      "\n",
      "Topic 4:\n",
      "need hello let way guy winner week\n",
      "\n",
      "Topic 5:\n",
      "kind head everyone moment everything hair runway\n",
      "\n",
      "Topic 6:\n",
      "lady work cash mean somebody feel something\n",
      "\n",
      "Topic 7:\n",
      "judge show stage bit decision way talk\n",
      "\n",
      "Topic 8:\n",
      "bit head today work life luck elimination\n",
      "\n",
      "Topic 9:\n",
      "let something face home hey lot bit\n"
     ]
    }
   ],
   "source": [
    "topics_num = 10\n",
    "words_num = 7\n",
    "\n",
    "topic_modelling_gensim(lda_corpus, dictionary, topics_num, words_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кое-что можно выделить. Этот алгоритм лучше выцепляет фразы, которые повторяются из эпизода в эпизод в одних и тех же контекстах и вообще вполне себе репрезентуют определенные темы.\n",
    "\n",
    "Например, тема победы в третьем топике - там разные куски из фразы \"Gentlemen, start your engines and may the best woman win!\".\n",
    "\n",
    "Тема судейства в седьмом топике - также куски типичных фраз \"While you untuck backstage, the judges and I will deliberate\"  и \"I made my decision\".\n",
    "\n",
    "Тема финального испытания каждого эпизода, в котором определяется, кто из королев уйдет, в 8 топике - также угадываются типичные для этого контекста фразы \"save yourself from elimination\", \"lipsync for your life\", \"good luck and don't fuck it up\".\n",
    "\n",
    "Попробуем 3 темы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "let way moment winner hello bit music\n",
      "\n",
      "Topic 1:\n",
      "something week life competition woman race honey\n",
      "\n",
      "Topic 2:\n",
      "home challenge thing year day woman winner\n"
     ]
    }
   ],
   "source": [
    "topics_num = 3\n",
    "words_num = 7\n",
    "\n",
    "topic_modelling_gensim(lda_corpus, dictionary, topics_num, words_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...нет, не особо осмысленно.\n",
    "\n",
    "Попробуем 25 тем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0:\n",
      "time woman winner hey way elimination let\n",
      "\n",
      "Topic 1:\n",
      "show everybody visage lady world thank kind\n",
      "\n",
      "Topic 2:\n",
      "work time winner face everybody hello home\n",
      "\n",
      "Topic 3:\n",
      "hell honey challenge hair face queen god\n",
      "\n",
      "Topic 4:\n",
      "day body time lady challenge ru honey\n",
      "\n",
      "Topic 5:\n",
      "hell bitch chance way hi hair judge\n",
      "\n",
      "Topic 6:\n",
      "competition home cash room day kind woman\n",
      "\n",
      "Topic 7:\n",
      "mean cash anything guy time man kind\n",
      "\n",
      "Topic 8:\n",
      "show guy thing hey hi life race\n",
      "\n",
      "Topic 9:\n",
      "life way somebody judge something music guy\n",
      "\n",
      "Topic 10:\n",
      "work race face music stage lot engine\n",
      "\n",
      "Topic 11:\n",
      "judge woman challenge feel thing lady life\n",
      "\n",
      "Topic 12:\n",
      "face stage world thank see music anything\n",
      "\n",
      "Topic 13:\n",
      "woman judge year deliberate talk today body\n",
      "\n",
      "Topic 14:\n",
      "lot show word bitch hello engine judge\n",
      "\n",
      "Topic 15:\n",
      "everything honey head runway somebody music day\n",
      "\n",
      "Topic 16:\n",
      "everything thank home week feel time day\n",
      "\n",
      "Topic 17:\n",
      "winner okay kind luck god impress moment\n",
      "\n",
      "Topic 18:\n",
      "life face feel week yeah tonight music\n",
      "\n",
      "Topic 19:\n",
      "winner today kind challenge guy bit part\n",
      "\n",
      "Topic 20:\n",
      "life okay music judge challenge body thing\n",
      "\n",
      "Topic 21:\n",
      "hell week cash stage bit life year\n",
      "\n",
      "Topic 22:\n",
      "lot race show stage music bit let\n",
      "\n",
      "Topic 23:\n",
      "week love head need life hey competition\n",
      "\n",
      "Topic 24:\n",
      "work thing today everything honey show bit\n"
     ]
    }
   ],
   "source": [
    "topics_num = 25\n",
    "words_num = 7\n",
    "\n",
    "topic_modelling_gensim(lda_corpus, dictionary, topics_num, words_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...а вот это уже малоосмысленно, видимо, слишком большое количество тем. Можно с натяжкой что-то повыделять, но это будет не так красиво, как в случае с 10 темами."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
